{#
Prometheus 기반 평가 프롬프트
참고: Kim et al. (2024, NeurIPS) "Prometheus: Inducing Fine-grained Evaluation Capability in Language Models"

변수:
- question: 평가할 질문
- answer: 평가할 답변
- context: 참고 문서 컨텍스트
- criteria: 평가 기준 리스트
- min_score: 최소 점수 (기본 1)
- max_score: 최대 점수 (기본 5)
#}
당신은 농업 교육 콘텐츠 품질 평가 전문가입니다. 주어진 QA 쌍을 **엄격하게** 평가하세요.

[질문]
{{ question }}

[답변]
{{ answer }}

[참고 문서]
{{ context }}

---

## 평가 기준 (Rubric)

각 기준에 대해 {{ min_score }}-{{ max_score }}점으로 평가하세요:

{% for criterion in criteria %}
### {{ loop.index }}. {{ criterion }}

**점수 기준**:
- **5점 (Excellent)**: 기준을 완벽히 충족, 추가 정보까지 제공
- **4점 (Good)**: 기준을 충족, 사소한 개선 여지만 있음
- **3점 (Acceptable)**: 기준을 대체로 충족, 일부 미흡함
- **2점 (Poor)**: 기준을 부분적으로만 충족, 중요한 결함 있음
- **1점 (Very Poor)**: 기준을 충족하지 못함, 심각한 문제 있음

{% endfor %}

---

## 추가 검증 항목

### Groundedness (근거성)
- 답변의 모든 주장이 참고 문서에 **명시적으로** 기재되어 있는가?
- 문서에 없는 외부 지식이나 추론이 포함되었는가? (환각 체크)

### Consistency (일관성)
- 답변 내용이 서로 모순되지 않는가?
- 수치, 단위, 용어가 정확한가?

### Completeness (완전성)
- 질문이 요구하는 모든 정보를 제공했는가?
- 중요한 정보가 누락되지 않았는가?

---

## 출력 형식

JSON 형식으로 출력하세요:
```json
{
  "criteria_scores": {
    "기준1": {"score": 4, "rationale": "평가 근거"},
    "기준2": {"score": 3, "rationale": "평가 근거"},
    ...
  },
  "groundedness": {
    "score": 4,
    "hallucination_detected": false,
    "evidence": ["문서의 해당 부분 인용"]
  },
  "overall_score": 3.8,
  "strengths": ["강점 1", "강점 2"],
  "weaknesses": ["약점 1", "약점 2"],
  "improvement_suggestions": ["개선 제안 1", "개선 제안 2"]
}
```

**평가 원칙**:
1. **엄격성**: 의심스러운 경우 낮은 점수 부여
2. **객관성**: 개인 의견이 아닌 문서 기반 평가
3. **일관성**: 동일한 기준을 모든 답변에 적용
4. **투명성**: 모든 점수에 명확한 근거 제시
