# RAG 기반 LLM-as-a-Judge 데이터셋 구축 설정
# 참고: Self-Instruct, Evol-Instruct, RAFT, LLM-as-a-Judge, Prometheus

llm:
  generator:
    # 질문/답변 생성용 (저비용 모델 권장)
    # OpenAI-compatible API 규격
    # NOTE: base_url은 환경변수 OPENAI_BASE_URL로 오버라이드됨 (.env 참조)
    base_url: "${OPENAI_BASE_URL}"
    model: "gemini-3-flash-preview"  # 빠른 생성용
    api_key: "${API_KEY}"
    temperature: 0.7
    max_tokens: 2048
  
  judge:
    # SOTA 평가용 (고성능 모델 권장)
    # OpenAI-compatible API 규격
    base_url: "${OPENAI_BASE_URL}"
    model: "claude-sonnet-4-5"  # 고품질 평가용
    api_key: "${API_KEY}"
    temperature: 0.0
    max_tokens: 1024

pipeline:
  # Self-Instruct 기반 설정 (Wang et al., 2023)
  seed_questions_per_chunk: 3
  question_diversity_threshold: 0.7
  
  # Evol-Instruct 기반 설정 (Xu et al., 2023)
  max_iterations: 3
  complexity_levels:
    - "basic"      # 단순 사실 질문
    - "intermediate"  # 관계 추론 질문
    - "advanced"   # 다단계 복합 질문
  
  # LLM-as-a-Judge 기반 설정 (Zheng et al., 2024)
  # Prometheus 루브릭 적용 (Kim et al., 2024)
  evaluation_criteria:
    - name: "groundedness"
      weight: 0.4
      description: "답변이 제공된 문서에만 기반하는가 (환각 방지)"
    - name: "accuracy"
      weight: 0.35
      description: "도메인 지식 측면에서 정확한가"
    - name: "completeness"
      weight: 0.25
      description: "질문에 충분한 정보를 제공하는가"
  
  pass_threshold: 4.5
  output_format: "jsonl"

rag:
  # RAFT 기반 설정 (Zhang et al., 2024)
  chunk_size: 512
  chunk_overlap: 50
  embedding_model: "sentence-transformers/all-MiniLM-L6-v2"
  vector_db: "chromadb"
  top_k: 5
  
  # PDF/이미지 처리 설정 (EasyOCR/PaddleOCR 기반 - 고성능/경량)
  use_ocr_fallback: true   # 텍스트 추출 실패 시 OCR 사용
  ocr_backend: "auto"      # auto(권장): DeepSeek > EasyOCR > PaddleOCR > Tesseract 순
  ocr_lang: "korean"       # OCR 언어 (korean, eng 등)
  ocr_dpi: 200             # OCR 이미지 해상도 (엣지 환경 고려)
  ocr_use_gpu: false       # 엣지 환경 기본 CPU 사용
  extract_tables: true     # PDF 테이블 추출 여부

  # DeepSeek-OCR 설정 (GPU 권장, 8GB VRAM 기준)
  deepseek_ocr:
    enabled: false              # DeepSeek-OCR 활성화 여부 (pip install .[deepseek] 필요)
    model_id: "deepseek-ai/DeepSeek-OCR"  # HuggingFace 모델 ID
    model_size: "tiny"          # tiny(64tok), small(100tok), base(256tok), large(400tok)
    quantize: "4bit"            # none, 8bit, 4bit (8GB VRAM은 4bit 권장)
    max_batch_size: 4           # 배치 크기 (VRAM에 따라 조절)
    compression_ratio: 10       # 압축 비율 (10x 권장, 97% 정확도)
    use_vllm: true              # vLLM 백엔드 사용 (권장)
    device: "cuda"              # cuda 또는 cpu

domain:
  name: "smartfarm"
  description: "스마트팜 도메인 (와사비, 토마토 등 작물 재배)"
  seed_questions:
    - "이 작물의 최적 재배 온도는?"
    - "양액 관리 시 주의사항은?"
    - "주요 병해충과 대처법은?"

# ---------------------------------------------------------------------------
# (Additive) Corpus / MT / MQM pipeline config for corpus_cli.py
# ---------------------------------------------------------------------------
corpus:
  # Official CGIAR datasets to export as EN corpus (JSON via `datasets`)
  cgiar_datasets:
    - "CGIAR/gardian-ai-ready-docs"
    - "CGIAR/cirad-ai-documents"
    - "CGIAR/ifpri-ai-documents"
  # Optional filter keywords for domain narrowing (leave empty to keep all)
  filter_keywords: []
  output_dir: "dataset/output"

translation:
  src_lang: "en"
  tgt_lang: "ko"
  # Optional glossary file (YAML/JSON mapping). Example:
  # glossary_path: "dataset/config/wasabi_glossary.yaml"
  glossary_path: ""

mqm:
  # Optional: multiple judges (e.g., Claude + Gemini).
  # If not provided, corpus_cli falls back to `llm.judge`.
  judges:
    - name: "claude"
      base_url: "${OPENAI_BASE_URL}"
      model: "claude-sonnet-4-5"
      api_key: "${API_KEY}"
      temperature: 0.0
      max_tokens: 1200
    - name: "gemini"
      base_url: "${OPENAI_BASE_URL}"
      model: "gemini-2.5-pro"
      api_key: "${API_KEY}"
      temperature: 0.0
      max_tokens: 1200
  # Example:
  # judges:
  #   - name: "gpt5"
  #     base_url: "https://api.openai.com/v1"
  #     model: "gpt-5"
  #     api_key: "${API_KEY}"
  #     temperature: 0.0
  #     max_tokens: 1200
  #   - name: "gemini3"
  #     base_url: "https://api.openai.com/v1"
  #     model: "gemini-3.0"
  #     api_key: "${API_KEY}"
  #     temperature: 0.0
  #     max_tokens: 1200

# 재현성 설정 (Reproducibility)
reproducibility:
  random_seed: 42  # 모든 랜덤 생성에 사용
  deterministic: true  # 결정론적 동작 (가능한 경우)
  log_prompts: true  # 프롬프트 로깅 (디버깅용)
  
  # LLM API seed 지원 (OpenAI, Anthropic 등)
  llm_seed: 42  # API 호출 시 seed 파라미터 전달
